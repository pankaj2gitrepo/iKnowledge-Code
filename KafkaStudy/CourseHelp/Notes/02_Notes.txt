* Producer
1. acks
   Step 1: Producer sends data to Leader Broker
   Step 2: Leader Broker receives data
   Step 3: Leader Broker sends data to replicas
   Step 4: All replicas including Leader Broker writes data to queue

   acks=0
   Producer won't wait for acknoledgement.

   acks=1
   Producer will wait for Leader acknowledgement. Leader Broker sends acknowledgement as soon as it receives data, i.e after Step 2.

   acks=-1 or acks=all
   Producer will wait for acknowledgement from Leader Broker as well as from replicas.
   Value from property "min.insync.replicas" is considered to wait for acks from how many replicas. This excludes Leader Broker.
   If acks=all and min.insync.replicas=1, then Leader Broker will send ack after sending data to one replica, i.e after Step 3
   If acks=all and min.insync.replicas=2, then Leader Broker will send ack after sending data to two replicas, i.e after Step 3

2. Producer retries
   If Producer setting expects ack and does not receive ack for the value mentioned at property "delivery.timeout.ms",
   record will be treated as failed and resent again.
   Producer will try retry upto value mentioned at "retries"

3. Idempotent Producer
   Step 1: Producer sends message
   Step 2: Broker receives message and commit
   Step 3: Broker sends ack to Producer
   Step 4: Producer sends next message
   ... continues

   If network failure happens at Step 3, while sending ack, the Producer will be sending same message again.
   In this case, Broker will do commit for same message, which leads duplicate commit.
   To avoid this scenario, just set property "enable.idempotence" to "true", which is default since Kafka 3.0
   In this case, Broker will not commit for duplicate message, and just send ack to Producer.

4. Message compression
   Broker Level Compression: Applies to all topics
   Topic Level Compression: Applies to only the mentioned topic

   - "compression.type"="producer" is a Broker Level Compression, Broker takes compressed batch and writes directly to topic's log file
   - "compression.type"="none" is a Broker Level Compression, Broker takes compressed batch, decompresses the messages and then writes to topic's log file
   - "compression.type"="lz4" is a Broker Level Compression, Broker takes compressed batch
     if compression type is matching with Producer, then data directly stores on disk as is
     if compression type is not matching with Producer, then Broker decompresses data, compresses it with Broker compression type, and then stores on disk
     
   If Broker compression is enabled, then it will consume extra CPU cycles.

5. linger.ms, max.in.flight.requests.per.connection and batch.size
   If max.in.flight.requests.per.connection is set to 5, then Producer will send max 5 messages in a batch.
   linger.ms is the time value, upto which Producer will wait to fill batch before sending.
   e.g Take a case when max.in.flight.requests.per.connection=5, linger.ms=100.
   If a batch is filled with 2 messages, then Producer will wait for 100ms to fill more messages into the batch before sending.

   batch.size is maximum number of bytes that will be included in a batch.
   A batch is allocated per partition.

6. buffer.memory and max.block.ms
   If Producer produces faster than Broker can take, the records will be buffered in memory
   upto the byte size mentioned at buffer.memory
   default buffer.memory=33554432, which is 32MB

   If buffer memory is full, then .send() method will start to block (won't return right away) for 
   the milliseond time mentioned at property max.block.ms
   default max.block.ms=60000, which is 60 seconds, means on overload buffer memory, 
   code at .send() will be blocked for 60 seconds, if after 60 seconds, buffer memory is still full, then 
   .send() method will throw exception.

7. Consumer Delivery Semantics
   - At most once: Offsets are committed as soon as message batch is received, i.e before processing.
   If something goes wrong after processing few messages from batch, the processing of messages after the 
   failure will be lost, as next time, consumer will be receiving message after last committed point.
   - At least once(prefered): Offsets are committed after processing all messages from batch.
   If something goes wrong after processing few messages from batch, commit will never happen.
   Next time, consumer will be receiving already processed message twice. In this case, the consumer processing 
   system should be kept as Idempotent to avoid impact on system if same message processed multiple times.
   - Exactly once: Can be achieved using transactional API

8. Auto commit
   - enable.auto.commit=true & auto.commit.interval.ms=5000
   Commit will be done at poll() method, after time value mentioned at auto.commit.interval.ms is passed.
   Consumer enables auto commits after every 5 seconds, at poll() method. If poll() method gets call at 4th second after last commit,
   it won't do commit, If poll() method gets call after 5 seconds from last commit, e.g at 6th second, the commit will be done that time.
   - enable.auto.commit=false
   Consumer has to take care call to commitAsync() manually.

9. Offset reset behaviour
   - auto.offset.reset=latest: consumer will read from end of log
   - auto.offset.reset=earliest: consumer will read from start of log
   - auto.offset.reset=none: consumer will throw exception if tries to read from log

10. Broker setting offset.retention.minutes
   Consumer offset will be lost, if consumer has not ready any message, after the value mentioned at offset.retention.minutes.

11. heartbeat.interval.ms and sesssion.timeout.ms
   One of consumer thread sends a heartbeat byte to Kafka Broker, after every time value mentioned at heartbeat.interval.ms. Default heartbeat.interval.ms is 3 seconds.
   If no heartbeat byte is received to Kafka Broker from a consumer for duration time mentioned in sesssion.timeout.ms, the consumer is considered as dead.
   Default session.timeout.ms is 45 seconds.
   sesssion.timeout.ms is useful to detect issue at cosumer.

12. max.poll.interval.ms
   Maximum time between two poll() calls, before declaring consumer as dead.
   Default max.poll.interval.ms is 5 minutes.
   max.poll.interval.ms is useful to detect data processing issue with the consumer.

13. max.poll.records
   Maximum number of records to receive per poll request.
   Default max.poll.records is 500.

14. fetch.min.bytes
   Minimun data bytes to pull on each request.
   Default fetch.min.bytes is 1.

15. fetch.max.wait.ms
   Maximum amount of time the Kafka Broker will block before answering the fetch request to immdediately satisfy 
   value from fetch.min.bytes.
   e.g. fetch.min.bytes=5 and fetch.max.wait.ms=500: Until the requirement of 5 bytes to be satisfied, 
   broker will wait for 500 milliseond of latency before the fetch returns data to consumer.
   Default fetch.max.wait.ms is 500.

16. max.partition.fetch.bytes
   Maximum amount of data per partition the server will return.
   Default max.partition.fetch.bytes is 1 MB.

17. fetch.max.bytes
   Maximum data returned for each request.
   Default fetch.max.bytes is 55 MB.